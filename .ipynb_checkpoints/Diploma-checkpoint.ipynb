{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import telethon.sync\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import aiohttp\n",
    "import aiomoex\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import mpld3\n",
    "import sys\n",
    "import time\n",
    "import pymorphy2\n",
    "from string import punctuation\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import scipy.stats\n",
    "# import spacy\n",
    "# from spacy.lang.ru.examples import sentences\n",
    "from deep_translator import (GoogleTranslator,\n",
    "                             PonsTranslator,\n",
    "                             LingueeTranslator,\n",
    "                             MyMemoryTranslator,\n",
    "                             YandexTranslator,\n",
    "                             DeepL,\n",
    "                             QCRI,\n",
    "                             single_detection,\n",
    "                             batch_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>–°–±–æ—Ä —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ —Ç–µ–ª–µ–≥—Ä–∞–º–º-–∫–∞–Ω–∞–ª–æ–≤</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_id = 1923089\n",
    "api_hash = '8e526e12df9137207e2f7b966924a246'\n",
    "\n",
    "client = telethon.sync.TelegramClient('Kostya3', api_id, api_hash)\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "#–°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ –ª–∏—Å—Ç—ã –∏ –∑–∞–¥–∞—ë–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –≤ –≤—ã–±–æ—Ä–∫–µ —Ä–∞–≤–Ω–æ–µ n*–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–ª–µ–≥—Ä–∞–º–º –∫–∞–Ω–∞–ª–æ–≤ \n",
    "texts =[]\n",
    "dates = []\n",
    "n=int(input())\n",
    "\n",
    "#–°–æ–±–∏—Ä–∞–µ–º –ø–æ—Å—Ç—ã –∏–∑ –∫–∞–Ω–∞–ª–∞ –†—ã–Ω–∫–∏–î–µ–Ω—å–≥–∏–í–ª–∞—Å—Ç—å\n",
    "async def rdv():\n",
    "    messages = await client.get_messages('https://t.me/AK47pfl', n)\n",
    "    i=0\n",
    "    while i<len(messages):\n",
    "        text = messages[i].text\n",
    "        date = messages[i].date\n",
    "        date = date.replace(tzinfo=None)\n",
    "        texts.append(text)\n",
    "        dates.append(date)\n",
    "        i+=1\n",
    "    return(texts, dates)\n",
    "async with client:\n",
    "    client.loop.run_until_complete(rdv())\n",
    "\n",
    "#–°–æ–±–∏—Ä–∞–µ–º –ø–æ—Å—Ç—ã –∏–∑ –∫–∞–Ω–∞–ª–∞ –°–∏–≥–Ω–∞–ª—ã –†–¶–ë\n",
    "async def signals():\n",
    "    messages = await client.get_messages('https://t.me/cbrstocks', n)\n",
    "    i=0\n",
    "    while i<len(messages):\n",
    "        text = messages[i].text\n",
    "        date = messages[i].date\n",
    "        date = date.replace(tzinfo=None)\n",
    "        texts.append(text)\n",
    "        dates.append(date)\n",
    "        i+=1\n",
    "    return(texts, dates)\n",
    "async with client:\n",
    "    client.loop.run_until_complete(signals())\n",
    "\n",
    "#–§–æ—Ä–º–∏—Ä—É–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö\n",
    "tab = pd.DataFrame({'–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è': dates, '–°–æ–æ–±—â–µ–Ω–∏–µ': texts})\n",
    "tab = tab.set_index('–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è')\n",
    "tab = tab.sort_index()\n",
    "tab.to_csv(\"raw_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>–°–±–æ—Ä —Ç–∏–∫–µ—Ä–æ–≤ –∞–∫—Ü–∏–π, —Ç–æ—Ä–≥—É–µ–º—ã—Ö –≤ —Ä–µ–∂–∏–º–µ TQBR</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>–î–ª—è –≤—ã–±–æ—Ä–∫–∏ –±—ã–ª–∏ –≤–∑—è—Ç—ã –Ω–µ –≤—Å–µ –∞–∫—Ü–∏–∏, —Ç–æ—Ä–≥—É—é—â–∏–µ—Å—è –Ω–∞ –ú–æ—Å–±–∏—Ä–∂–µ, –∞ –ª–∏—à—å 268 –∞–∫—Ü–∏–π, —Ç–æ—Ä–≥—É–µ–º—ã—Ö –≤ —Ä–µ–∂–∏–º–µ T+0</em>\n",
    "<p><em>–≠—Ç–æ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∞–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≤ —Å–æ—Ü—Å–µ—Ç—è—Ö</em></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–¥–æ—Å—Ç–∞–µ–º —Ç–∏–∫–µ—Ä—ã –∞–∫—Ü–∏–π –∏ –∏—Ö –Ω–∞–∑–≤–∞–Ω–∏—è —Å —Å–∞–π—Ç–∞ –º–æ—Å–±–∏—Ä–∂–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ç–∞–±–ª–∏—Ü—É\n",
    "async def shares_func():\n",
    "    request_url = \"https://iss.moex.com/iss/engines/stock/markets/shares/boards/TQBR/securities.json\"\n",
    "    arguments = {\"securities.columns\": (\"SECID,\" \"SHORTNAME,\" \"SECNAME\")}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        iss = aiomoex.ISSClient(session, request_url, arguments)\n",
    "        data = await iss.get()\n",
    "        shares = pd.DataFrame(data[\"securities\"])\n",
    "        shares.to_csv('shares')\n",
    "\n",
    "asyncio.run(shares_func())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shares = pd.read_csv('shares')\n",
    "shares = shares.drop('Unnamed: 0', axis = 'columns')\n",
    "shares['NewName'] = shares.SHORTNAME.map(lambda x: x[0: x.find(' ')] if ' ' in x else x)\n",
    "shares['NewName'] = shares.NewName.map(lambda x: x[0: x.find('-')] if '-' in x else x)\n",
    "shares['NewName'] = shares.NewName.map(lambda x: x[0: x.find('.')] if '.' in x else x)\n",
    "shares['NewName'] = shares['NewName'].replace('–°–∏—Å—Ç–µ–º–∞', '–ê–§–ö –°–∏—Å—Ç–µ–º–∞')\n",
    "shares['NewName'] = shares['NewName'].replace('FIVE', 'X5 RetailGroup')\n",
    "shares['NewName'] = shares['NewName'].replace('FIXP', 'FixPrice')\n",
    "shares['NewName'] = shares['NewName'].replace('Yandex', '–Ø–Ω–¥–µ–∫—Å')\n",
    "shares['NewName'] = shares['NewName'].replace('+–ú–æ—Å–≠–Ω–µ—Ä–≥–æ', '–ú–æ—Å–≠–Ω–µ—Ä–≥–æ')\n",
    "secid = list(shares['SECID'])\n",
    "shortname = list(shares['NewName'])   \n",
    "secid_and_shortname = dict(zip(secid, shortname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>–í –¥–∞–Ω–Ω–æ–π —á–∞—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã –≤–µ–¥—ë—Ç—Å—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Ç–∞, –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –∏–∑–±–∞–≤–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –∏—Å–∫–∞–∂–µ–Ω–∏–π</p>\n",
    "<p>–û—Å–Ω–æ–≤–Ω—ã–µ —à–∞–≥–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏:</p>\n",
    "<ol>\n",
    "    <li>–£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤</li>\n",
    "    <li>–ó–∞–º–µ–Ω–∞ —Ç–∏–∫–µ—Ä–æ–≤ –Ω–∞ –∫—Ä–∞—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —É–¥–æ–±–Ω–æ –∏—Å–∫–∞—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ</li>\n",
    "    <li>–£–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –∏ —Ö—ç—à—Ç–µ–≥–æ–≤</li>\n",
    "    <li>–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ –≤—Ä—É—á–Ω—É—é –ø—É—Ç—ë–º –ø—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ, –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ç–æ–Ω–∞</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "raw_data = pd.read_csv(\"raw_text\")\n",
    "raw_data = raw_data.set_index('–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è')\n",
    "raw_data = raw_data.dropna()\n",
    "\n",
    "#—É–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏\n",
    "raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'] = raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'].str.replace(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|\\\n",
    "                                                        (?:%[0-9a-fA-F][0-9a-fA-F]))+\", '', case=False) \n",
    "# —É–¥–∞–ª—è–µ–º —ç–º–æ–¥–∑–∏\n",
    "raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ']=raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'].str.replace(\"[\"u\"\\U0001F600-\\U0001F64F\"  \n",
    "                    u\"\\U0001F300-\\U0001F5FF\" \n",
    "                    u\"\\U0001F680-\\U0001F6FF\" \n",
    "                    u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "                    u\"\\U00002500-\\U00002BEF\" \n",
    "                    u\"\\U00002702-\\U000027B0\"\n",
    "                    u\"\\U00002702-\\U000027B0\"\n",
    "                    u\"\\U000024C2-\\U0001F251\"\n",
    "                    u\"\\U0001f926-\\U0001f937\"\n",
    "                    u\"\\U00010000-\\U0010ffff\"\n",
    "                    u\"\\u2640-\\u2642\"\n",
    "                    u\"\\u2600-\\u2B55\"\n",
    "                    u\"\\u200d\"\n",
    "                    u\"\\u23cf\"\n",
    "                    u\"\\u23e9\"\n",
    "                    u\"\\u231a\"\n",
    "                    u\"\\ufe0f\"\n",
    "                    u\"\\u3030\"\n",
    "                    \"]+\", \"\", regex = True)\n",
    "\n",
    "#–£–¥–∞–ª—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –º—É—Å–æ—Ä\n",
    "raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ']=raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'].str.replace('[*()@,__‚Ä¢#]', \" \", regex = True)\n",
    "raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ']=raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'].str.replace('\\n\\n', \" \", regex = True)\n",
    "raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ']=raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'].str.replace('[\\n]', \"\", regex = True)\n",
    "raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ']=raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'].str.replace('  ', \" \", regex = True)\n",
    "raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ']=raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'].str.replace('AK47pfl', \"\", regex = True)\n",
    "raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ']=raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'].str.replace('[*[(+\\\\\\]\\t]', \"\", regex = True)\n",
    "raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ']=raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'].str.replace('[:]', \"\", regex = True)\n",
    "raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ']=raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'].str.replace('[%]', \"–ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤\", regex = True)\n",
    "raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ']=raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'].str.replace('[$\"]', \"–¥–æ–ª–ª–∞—Ä–æ–≤\", regex = True)\n",
    "raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ']=raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'].str.replace('–¥–æ–ª–ª–∞—Ä–æ–≤', \" \", regex = True)\n",
    "raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ']=raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'].str.replace('AK47PFLCHAT', \"\", regex = True)\n",
    "raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ']=raw_data['–°–æ–æ–±—â–µ–Ω–∏–µ'].str.replace('\\u200b', \"\", regex = True)\n",
    "raw_data[\"–°–æ–æ–±—â–µ–Ω–∏–µ\"] = raw_data[\"–°–æ–æ–±—â–µ–Ω–∏–µ\"].str.replace('[', \"\", regex = True)\n",
    "raw_data[\"–°–æ–æ–±—â–µ–Ω–∏–µ\"] = raw_data[\"–°–æ–æ–±—â–µ–Ω–∏–µ\"].str.replace(']', \"\", regex = True)\n",
    "raw_data[\"–°–æ–æ–±—â–µ–Ω–∏–µ\"] = raw_data[\"–°–æ–æ–±—â–µ–Ω–∏–µ\"].str.replace('RDVPREMIUMbot', \"\", regex = True)\n",
    "\n",
    "# –∑–∞–º–µ–Ω—è–µ–º —Ç–∏–∫–µ—Ä—ã –Ω–∞ –∫—Ä–∞—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è\n",
    "raw_data[\"–°–æ–æ–±—â–µ–Ω–∏–µ\"] = raw_data[\"–°–æ–æ–±—â–µ–Ω–∏–µ\"].replace(secid_and_shortname, regex=True)\n",
    "\n",
    "# —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "raw_data = raw_data.dropna()\n",
    "raw_data.to_csv(\"cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>–í—ã–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ü–µ–Ω–∞–º</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>–î–∞–Ω–Ω—ã–µ –≥—Ä—É–∑—è—Ç—Å—è –∑–∞ 4 –≥–æ–¥–∞, –ø–æ—ç—Ç–æ–º—É –Ω–µ –≤—Å–µ –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ–ø–∞–¥—É—Ç –≤ –≤—ã–±–æ—Ä–∫—É, –æ–¥–Ω–∞–∫–æ —ç—Ç–æ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ—Å—Ç—ã–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –ø–µ—Ä–∏–æ–¥–∞</p>\n",
    "<p>–ë—ã–ª–æ —Ä–µ—à–µ–Ω–æ —Å–æ–∑–¥–∞—Ç—å –º–∞—Å—Å–∏–≤ –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤—Å–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–º –∫–æ–º–ø–∞–Ω–∏—è–º, —á—Ç–æ–±—ã –Ω–µ –≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–∞–∂–¥–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –æ—Ç–¥–µ–ª—å–Ω–æ</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for name in shares['SECID']:\n",
    "    d[name] = yf.download(name + '.ME', '2017-04-01', '2021-09-30') #–ó–¥–µ—Å—å –º–µ–Ω—è–µ–º –ø–µ—Ä–∏–æ–¥\n",
    "    if d.get(name).empty == True:\n",
    "        d.pop(name)\n",
    "    else:\n",
    "        for i in range(0,d.get(name).shape[0]-1):\n",
    "            d.get(name).loc[d.get(name).index[i+1],'Rit'] = np.log(d.get(name).iloc[i+1, 4]/d.get(name).iloc[i, 4])\n",
    "        d.get(name)['Rit_norm'] = d.get(name)['Rit'].rolling(window=60).mean()\n",
    "        d.get(name)['ùúÄ'] = (d.get(name)['Rit'] - d.get(name)['Rit_norm'])*100\n",
    "        d.get(name)['AR'] = d.get(name)['ùúÄ'].rolling(window=60, min_periods=1).mean()\n",
    "        d.get(name)['CAR'] = d.get(name)['AR'].rolling(window=15, center=True).sum()\n",
    "        #–°—á–∏—Ç–∞–µ–º t-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
    "        d.get(name)['for_s2'] = (d.get(name)['ùúÄ']-d.get(name)['AR'])**2/59\n",
    "        d.get(name)['s2'] = d.get(name)['for_s2'].rolling(window=60, min_periods=1).sum()\n",
    "        d.get(name)['t'] = d.get(name)['AR']/(((d.get(name)['s2']/60))**(1/2))\n",
    "        #–í—ã–≥—Ä—É–∂–∞–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "        d.get(name)['t_crit_left_001'] = scipy.stats.t.ppf(0.01, 59)\n",
    "        d.get(name)['t_crit_right_001'] = scipy.stats.t.ppf(1-0.01, 59)\n",
    "        d.get(name)['t_crit_left_005'] = scipy.stats.t.ppf(0.05, 59)\n",
    "        d.get(name)['t_crit_right_005'] = scipy.stats.t.ppf(1-0.05, 59)\n",
    "        d.get(name)['t_crit_left_01'] = scipy.stats.t.ppf(0.1, 59)\n",
    "        d.get(name)['t_crit_right_01'] = scipy.stats.t.ppf(1-0.1, 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –≤ —Å–æ—Ü—Å–µ—Ç—è—Ö</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>–ü–æ–¥—Å—á–µ—Ç –≤–µ–¥–µ—Ç—Å—è –ø–æ –≤—Å–µ–º –∫–æ–º–ø–∞–Ω–∏—è–º</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('cleaned_data.csv')\n",
    "data['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è']=pd.to_datetime(data['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è']).apply(lambda x: x.date())\n",
    "for key in d.keys():\n",
    "    Comp_name = shares[shares['SECID'] == key]['NewName'].values[0]\n",
    "    mention = data[data[\"–°–æ–æ–±—â–µ–Ω–∏–µ\"].str.contains(Comp_name, na=False)]\n",
    "    data['is' + key] = 0\n",
    "    for i in mention.index:\n",
    "        data.loc[data.index[i],'is'+key] = 1\n",
    "data.to_csv('maped_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt={}\n",
    "for key in d.keys():\n",
    "    cnt[key] = pd.pivot_table(data, values=('is' + key), index=['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è'], aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>–ü–µ—Ä–µ–≤–æ–¥ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ä—É—Å—Å–∫–æ–≥–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π</h3>\n",
    "<p>–≠—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –±—ã–ª–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å VADER-–∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = pd.read_csv('maped_data.csv')\n",
    "tr_data=tr_data.drop('Unnamed: 0', axis = 'columns')\n",
    "tr_data = tr_data.drop(tr_data[tr_data['–°–æ–æ–±—â–µ–Ω–∏–µ']==' '].index)\n",
    "tr_data = tr_data.reset_index()\n",
    "tr_data['message'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, tr_data.shape[0]-1):\n",
    "    print(i)\n",
    "    try:\n",
    "        tr_data.iloc[i, tr_data.columns.get_loc('message')]=(\n",
    "            GoogleTranslator(source='auto', target='en').translate(text=tr_data.iloc[i, tr_data.columns.get_loc('–°–æ–æ–±—â–µ–Ω–∏–µ')]))\n",
    "    except:\n",
    "        tr_data.iloc[i, tr_data.columns.get_loc('message')] = ''\n",
    "    clear_output()\n",
    "# tr_data.to_csv('filename.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞</h3>\n",
    "<p>–ù–∞ –¥–∞–Ω–Ω–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞, —É –∫–æ—Ç–æ—Ä–æ–≥–æ –µ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:</p>\n",
    "<ol>\n",
    "    <li>–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã => —Ç–µ—Ä—è–µ—Ç—Å—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ –ø–µ—Ä–µ–≤–æ–¥–∞</li>\n",
    "    <li>VADER –±—ã–ª –æ–±—É—á–µ–Ω –Ω–µ –Ω–∞ —Å—Ç–æ–ª—å —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>\n",
    "    <li>–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å —É—á—ë—Ç–æ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏</li>\n",
    "</ol>\n",
    "<p>–¢–µ–º –Ω–µ –º–µ–Ω–µ–µ —Ç–∞–∫–æ–π –º–µ—Ç–æ–¥ –¥–∞—ë—Ç –Ω–µ —Å–∞–º—É—é –ø–ª–æ—Ö—É—é —Ç–æ—á–Ω–æ—Å—Ç—å 57% –≤–µ—Ä–Ω–æ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å —É—á—ë—Ç–æ–º –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –µ–≥–æ –Ω–µ –Ω—É–∂–Ω–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–±—É—á–∞—Ç—å => –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª—å—à–∞—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞.</p>\n",
    "<p>–û –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏:</p>\n",
    "<ol>\n",
    "    <li>–ë—ã–ª–æ –ø—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ, –∏—Å—Ö–æ–¥—è –∏–∑ —É—Ä–∞–≤–Ω–µ–Ω–∏—è Score_neu|pos*compound*10</li>\n",
    "    <li>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å, –Ω–æ –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏–ª–∞, —Å –∫–æ—Ç–æ—Ä–æ–π –ø–æ–¥–∞—ë—Ç—Å—è —Ç–µ–∫—Å—Ç</li>\n",
    "    <li>–û—Ç—Å–µ–∫–∞—é—â–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –±—ã–ª–æ –≤—ã–±—Ä–∞–Ω–æ +-0.75 –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–≥–æ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ —Å—á—ë—Ç–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ</li>\n",
    "    <li>–í—Å—ë, —á—Ç–æ –≤–Ω—É—Ç—Ä–∏ —ç—Ç–æ–≥–æ –æ—Ç—Ä–µ–∑–∫–∞, —Å—á–∏—Ç–∞–µ—Ç—Å—è –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–º</li>\n",
    "    <li>–°—á—ë—Ç –ø—Ä–∏—Å—É–∂–¥–∞–µ—Ç—Å—è –ª—é–±–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –º–æ–¥–µ–ª–∏</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_data = pd.read_csv('tr_data.csv')\n",
    "tran_data=tran_data.drop('Unnamed: 0', axis = 'columns')\n",
    "tran_data = tran_data.drop(tran_data[tran_data['message']==' '].index)\n",
    "tran_data = tran_data.dropna()\n",
    "tran_data = tran_data.reset_index()\n",
    "tran_data=tran_data.drop('index', axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "neg = []\n",
    "neu = []\n",
    "pos = []\n",
    "comp = []\n",
    "for i in range(0, tran_data.shape[0]):\n",
    "    translated=tran_data['message'][i]\n",
    "    neg.append(sia.polarity_scores(translated).get('neg'))\n",
    "    neu.append(sia.polarity_scores(translated).get('neu'))\n",
    "    pos.append(sia.polarity_scores(translated).get('pos'))\n",
    "    comp.append(sia.polarity_scores(translated).get('compound'))\n",
    "scores = pd.DataFrame({'neg': neg, 'neu': neu, 'pos': pos, 'comp': comp})\n",
    "scores['score'] = 0\n",
    "for i in range(0,scores.shape[0]):\n",
    "    if scores.iloc[i,scores.columns.get_loc('neg')]*scores.iloc[i,scores.columns.get_loc('comp')]*10<-0.75:\n",
    "        scores.loc[scores.index[i],'score']=-1\n",
    "    elif scores.iloc[i,scores.columns.get_loc('pos')]*scores.iloc[i,scores.columns.get_loc('comp')]*10>0.75:\n",
    "        scores.loc[scores.index[i],'score']=1\n",
    "    else:\n",
    "        scores.loc[scores.index[i],'score']=0\n",
    "tran_data['neg_score'] = 0\n",
    "tran_data['neu_score'] = 0\n",
    "tran_data['pos_score'] = 0\n",
    "for i in scores[scores['score']==-1].index:\n",
    "        tran_data.loc[tran_data.index[i],'neg_score'] = 1\n",
    "for i in scores[scores['score']==0].index:\n",
    "        tran_data.loc[tran_data.index[i],'neu_score'] = 1\n",
    "for i in scores[scores['score']==1].index:\n",
    "        tran_data.loc[tran_data.index[i],'pos_score'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_neg_score = pd.pivot_table(tran_data, values=('neg_score'), index=['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è'], aggfunc=np.sum)\n",
    "cnt_neu_score = pd.pivot_table(tran_data, values=('neu_score'), index=['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è'], aggfunc=np.sum)\n",
    "cnt_pos_score = pd.pivot_table(tran_data, values=('pos_score'), index=['–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è'], aggfunc=np.sum)\n",
    "\n",
    "cnt_neg_score.index=pd.to_datetime(cnt_neg_score.index)\n",
    "cnt_neu_score.index=pd.to_datetime(cnt_neu_score.index)\n",
    "cnt_pos_score.index=pd.to_datetime(cnt_pos_score.index)\n",
    "\n",
    "cnt_score=pd.DataFrame({'neg': cnt_neg_score['neg_score'], 'neu': cnt_neu_score['neu_score'], \n",
    "                        'pos': cnt_pos_score['pos_score']}, index=cnt_neg_score.index)\n",
    "\n",
    "cnt_score['d_neg'] = cnt_score['neg']/(cnt_score['neg']+cnt_score['neu']+cnt_score['pos'])\n",
    "cnt_score['d_neu'] = cnt_score['neu']/(cnt_score['neg']+cnt_score['neu']+cnt_score['pos'])\n",
    "cnt_score['d_pos'] = cnt_score['pos']/(cnt_score['neg']+cnt_score['neu']+cnt_score['pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>–ì—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('–í–≤–µ–¥–∏—Ç–µ —Ç–∏–∫–µ—Ä –∞–∫—Ü–∏–∏')\n",
    "company = str(input())\n",
    "print('–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞:')\n",
    "print('–° –∫–∞–∫–æ–π –¥–∞—Ç—ã –≤—ã —Ö–æ—Ç–µ–ª–∏ –±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É?')\n",
    "period_start = str(input())\n",
    "print('–ü–æ –∫–∞–∫—É—é –¥–∞—Ç—É –≤—ã —Ö–æ—Ç–µ–ª–∏ –±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É?')\n",
    "period_end = str(input())\n",
    "\n",
    "if datetime.strptime(period_end, '%Y-%m-%d').date()<datetime.strptime(period_start, '%Y-%m-%d').date():\n",
    "    print('–ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –ø–µ—Ä–∏–æ–¥–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –±–æ–ª—å—à–µ –ø–µ—Ä–≤–æ–π')\n",
    "else:\n",
    "    idx = cnt.get(company)[cnt.get(company)['is'+company]>0].index.to_list()\n",
    "    \n",
    "    #—Ü–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è\n",
    "    plt.figure(figsize = (15, 8))\n",
    "    plt.plot(d.get(company)[period_start:period_end]['Adj Close'], color = \"black\", label = company + \" close price\")\n",
    "    plt.ylabel('–¶–µ–Ω–∞')\n",
    "    plt.xlabel('–î–∞—Ç–∞')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #–£–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏\n",
    "    plt.figure(figsize = (15, 3))\n",
    "    plt.plot(cnt.get(company)[datetime.strptime(period_start, '%Y-%m-%d').date():\n",
    "                              datetime.strptime(period_end,'%Y-%m-%d').date()]['is'+company], \n",
    "                              color='red', linewidth=2, label = '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –≤ —Ç–µ–ª–µ–≥—Ä–∞–º–º –∫–∞–Ω–∞–ª–∞—Ö')\n",
    "    plt.ylabel('–ö–æ–ª-–≤–æ')\n",
    "    plt.xlabel('–î–∞—Ç–∞')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #–î–æ–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö/–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö/–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π\n",
    "    plt.figure(figsize = (15, 3))\n",
    "    plt.plot(cnt_score[cnt_score.index.isin(idx)==True][period_start:period_end]['d_neg'], \n",
    "                              color='blue', linewidth=2, label = 'negative')\n",
    "    plt.plot(cnt_score[cnt_score.index.isin(idx)==True][period_start:period_end]['d_neu'], \n",
    "                              color='black', linewidth=2, label = 'neutral')\n",
    "    plt.plot(cnt_score[cnt_score.index.isin(idx)==True][period_start:period_end]['d_pos'], \n",
    "                              color='red', linewidth=2, label = 'positive')\n",
    "    plt.ylabel('–î–æ–ª—è')\n",
    "    plt.xlabel('–î–∞—Ç–∞')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()    \n",
    "    \n",
    "    #–ê–Ω–æ–º–∞–ª—å–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å\n",
    "    plt.figure(figsize = (15, 3))\n",
    "    plt.plot(d.get(company)[period_start:period_end]['ùúÄ'], color = \"blue\", label = company + \" anomalies\")\n",
    "    plt.ylabel('%')\n",
    "    plt.xlabel('–î–∞—Ç–∞')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #–°—Ä–µ–¥–Ω—è—è –∞–Ω–æ–º–∞–ª—å–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å\n",
    "    plt.figure(figsize = (15, 3))\n",
    "    plt.plot(d.get(company)[period_start:period_end]['AR'], color = \"black\", label = company + \" AR\")\n",
    "    plt.ylabel('%')\n",
    "    plt.xlabel('–î–∞—Ç–∞')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #–ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –∞–Ω–æ–º–∞–ª—å–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å\n",
    "    plt.figure(figsize = (15, 3))\n",
    "    plt.plot(d.get(company)[period_start:period_end]['CAR'], color = \"black\", label = company + \" CAR\")\n",
    "    plt.ylabel('%')\n",
    "    plt.xlabel('–î–∞—Ç–∞')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (15, 5))\n",
    "    plt.plot(d.get(company)[period_start:period_end]['t'], color = \"green\", \n",
    "             label = company + \" t-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\")\n",
    "    plt.plot(d.get(company)[period_start:period_end]['t_crit_left_001'], color = \"red\", \n",
    "             label = company + \" t-–∫—Ä–∏—Ç 0.01\")\n",
    "    plt.plot(d.get(company)[period_start:period_end]['t_crit_right_001'], color = \"red\", \n",
    "             label = company + \" t-–∫—Ä–∏—Ç 0.99\")\n",
    "    plt.plot(d.get(company)[period_start:period_end]['t_crit_left_005'], color = \"blue\", \n",
    "             label = company + \" t-–∫—Ä–∏—Ç 0.05\")\n",
    "    plt.plot(d.get(company)[period_start:period_end]['t_crit_right_005'], color = \"blue\", \n",
    "             label = company + \" t-–∫—Ä–∏—Ç 0.95\")\n",
    "    plt.plot(d.get(company)[period_start:period_end]['t_crit_left_01'], color = \"black\", \n",
    "             label = company + \" t-–∫—Ä–∏—Ç 0.1\")\n",
    "    plt.plot(d.get(company)[period_start:period_end]['t_crit_right_01'], color = \"black\", \n",
    "             label = company + \" t-–∫—Ä–∏—Ç 0.9\")\n",
    "    plt.ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ t-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏')\n",
    "    plt.xlabel('–î–∞—Ç–∞')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    ment = cnt.get(company)[datetime.strptime(period_start, '%Y-%m-%d').date():\n",
    "                              datetime.strptime(period_end,'%Y-%m-%d').date()]['is' + company]\n",
    "    price = d.get(company)[period_start:period_end]['Adj Close']\n",
    "    t = d.get(company)[period_start:period_end]['t']\n",
    "    df1 = pd.DataFrame({'mentions': ment, 'price': price})\n",
    "    df2 = pd.DataFrame({'mentions': ment, 't-stat': t})\n",
    "    \n",
    "    print('–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Ü–µ–Ω—ã –∑–∞–∫—Ä—ã—Ç–∏—è –∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π:\\n',df1.corr())\n",
    "    print('–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π t-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π:\\n',df2.corr())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
